{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload \n",
    "%autoreload 2\n",
    "from cchess import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random \n",
    "import time\n",
    "from utils import Dataset,ProgressBar\n",
    "from tflearn.data_flow import DataFlow,DataFlowStatus,FeedDictFlow\n",
    "from tflearn.data_utils import Preloader,ImagePreloader\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "from game_convert import convert_game\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a network predict select and move of Chinese chess, with minimal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GPU_CORE = 0\n",
    "BATCH_SIZE = 256\n",
    "BEGINING_LR = 0.01\n",
    "#TESTIMG_WIDTH = 500\n",
    "model_name = '11_8_resnet'\n",
    "data_dir = 'data/imsa-cbf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_winner(file,data_dir = 'data/imsa-play/'):\n",
    "    filename = os.path.join(data_dir,file)\n",
    "    with open(filename) as fhdl:\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ElePreloader(object):\n",
    "    def __init__(self,datafile,batch_size=64):\n",
    "        self.batch_size=batch_size\n",
    "        content = pd.read_csv(datafile,header=None,index_col=None)\n",
    "        self.filelist = [i[0] for i in content.get_values()]\n",
    "        self.pos = 0\n",
    "        self.feature_list = {\"red\":['A', 'B', 'C', 'K', 'N', 'P', 'R']\n",
    "                             ,\"black\":['a', 'b', 'c', 'k', 'n', 'p', 'r']}\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_iter = self.__iter()\n",
    "        assert(len(self.filelist) > batch_size)\n",
    "        self.game_iterlist = [None for i in self.filelist]\n",
    "    \n",
    "    def __iter(self):\n",
    "        retx1,rety1,retx2,rety2 = [],[],[],[]\n",
    "        filelist = []\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.game_iterlist[i] == None:\n",
    "                    if len(filelist) == 0:\n",
    "                        filelist = copy.copy(self.filelist)\n",
    "                        random.shuffle(filelist)\n",
    "                    self.game_iterlist[i] = convert_game(filelist.pop(),feature_list=self.feature_list)\n",
    "                game_iter = self.game_iterlist[i]\n",
    "                \n",
    "                try:\n",
    "                    x1,y1,x2,y2 = game_iter.__next__()\n",
    "                    x1 = np.transpose(x1,[1,2,0])\n",
    "                    x2 = np.transpose(x2,[1,2,0])\n",
    "                    x1 = np.expand_dims(x1,axis=0)\n",
    "                    x2 = np.expand_dims(x2,axis=0)\n",
    "                    retx1.append(x1)\n",
    "                    rety1.append(y1)\n",
    "                    retx2.append(x2)\n",
    "                    rety2.append(y2)\n",
    "                    if len(retx1) >= self.batch_size:\n",
    "                        yield (np.concatenate(retx1,axis=0),np.asarray(rety1)\n",
    "                               ,np.concatenate(retx2,axis=0),np.asarray(rety2))\n",
    "                        retx1,rety1,retx2,rety2 = [],[],[],[]\n",
    "                except :\n",
    "                    self.game_iterlist[i] = None\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        \n",
    "        x1,y1,x2,y2 = self.batch_iter.__next__()\n",
    "        return x1,y1,x2,y2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def res_block(inputx,name,training,block_num=2,filters=256,kernel_size=(3,3)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_res_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bn{}\".format(name,i))\n",
    "        if i == block_num - 1:\n",
    "            net = net + inputx #= tf.concat((inputx,net),axis=-1)\n",
    "        net = tf.nn.elu(net,name=\"{}_res_elu{}\".format(name,i))\n",
    "    return net\n",
    "\n",
    "def conv_block(inputx,name,training,block_num=1,filters=2,kernel_size=(1,1)):\n",
    "    net = inputx\n",
    "    for i in range(block_num):\n",
    "        net = tf.layers.conv2d(net,filters=filters,kernel_size=kernel_size,activation=None,name=\"{}_convblock_conv{}\".format(name,i),padding='same')\n",
    "        net = tf.layers.batch_normalization(net,training=training,name=\"{}_convblock_bn{}\".format(name,i))\n",
    "        net = tf.nn.elu(net,name=\"{}_convblock_elu{}\".format(name,i))\n",
    "    # net [None,10,9,2]\n",
    "    netshape = net.get_shape().as_list()\n",
    "    print(\"inside conv block {}\".format(str(netshape)))\n",
    "    net = tf.reshape(net,shape=(-1,netshape[1] * netshape[2] * netshape[3]))\n",
    "    net = tf.layers.dense(net,10 * 9,name=\"{}_dense\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_elu\".format(name))\n",
    "    return net\n",
    "\n",
    "def res_net_board(inputx,name,training,filters=256):\n",
    "    net = inputx\n",
    "    net = tf.layers.conv2d(net,filters=filters,kernel_size=(3,3),activation=None,name=\"{}_res_convb\".format(name),padding='same')\n",
    "    net = tf.layers.batch_normalization(net,training=training,name=\"{}_res_bnb\".format(name))\n",
    "    net = tf.nn.elu(net,name=\"{}_res_elub\".format(name))\n",
    "    for i in range(NUM_RES_LAYERS):\n",
    "        net = res_block(net,name=\"{}_layer_{}\".format(name,i + 1),training=training)\n",
    "        print(net.get_shape().as_list())\n",
    "    print(\"inside res net {}\".format(str(net.get_shape().as_list())))\n",
    "    net_unsoftmax = conv_block(net,name=\"{}_conv\".format(name),training=training)\n",
    "    return net_unsoftmax\n",
    "\n",
    "def get_scatter(name):\n",
    "    with tf.variable_scope(\"Test\"):\n",
    "        ph = tf.placeholder(tf.float32,name=name)\n",
    "        op = tf.summary.scalar(name,ph)\n",
    "    return ph,op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "inside res net [None, 10, 9, 256]\n",
      "inside conv block [None, 10, 9, 2]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "[None, 10, 9, 256]\n",
      "inside res net [None, 10, 9, 256]\n",
      "inside conv block [None, 10, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "NUM_RES_LAYERS = 10\n",
    "\n",
    "with tf.device(\"/gpu:{}\".format(GPU_CORE)):\n",
    "    X1 = tf.placeholder(tf.float32,[None,10,9,14])\n",
    "    y1 = tf.placeholder(tf.float32,[None,10,9])\n",
    "    X2 = tf.placeholder(tf.float32,[None,10,9,15])\n",
    "    y2 = tf.placeholder(tf.float32,[None,10,9])\n",
    "    \n",
    "    training = tf.placeholder(tf.bool,name='training_mode')\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    net_unsoftmax1 = res_net_board(X1,\"selectnet\",training=training)\n",
    "    net_unsoftmax2 = res_net_board(X2,\"movenet\",training=training)\n",
    "    \n",
    "    target1 = tf.reshape(y1,(-1,10 * 9))\n",
    "    target2 = tf.reshape(y2,(-1,10 * 9))\n",
    "    with tf.variable_scope(\"Loss\"):\n",
    "        loss_select = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target1,logits=net_unsoftmax1))\n",
    "        loss_move = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target2,logits=net_unsoftmax2))\n",
    "        loss = loss_select + loss_move\n",
    "        \n",
    "        loss_select_summary = tf.summary.scalar(\"loss_select\",loss_select)\n",
    "        loss_move_summary = tf.summary.scalar(\"loss_move\",loss_move)\n",
    "        loss_summary = tf.summary.scalar(\"total_loss\",loss)\n",
    "    net_softmax1 = tf.nn.softmax(net_unsoftmax1)\n",
    "    net_softmax2 = tf.nn.softmax(net_unsoftmax2)\n",
    "    \n",
    "    correct_prediction1 = tf.equal(tf.argmax(target1,1), tf.argmax(net_softmax1,1))\n",
    "    correct_prediction2 = tf.equal(tf.argmax(target2,1), tf.argmax(net_softmax2,1))\n",
    "    \n",
    "    with tf.variable_scope(\"Accuracy\"):\n",
    "        accuracy_select = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))\n",
    "        accuracy_move = tf.reduce_mean(tf.cast(correct_prediction2, tf.float32))\n",
    "        accuracy_total = accuracy_select * accuracy_move\n",
    "        \n",
    "        acc_select_summary = tf.summary.scalar(\"accuracy_select\",accuracy_select)\n",
    "        acc_move_summary = tf.summary.scalar(\"accuracy_move\",accuracy_move)\n",
    "        acc_summary = tf.summary.scalar(\"acc_summary\",accuracy_total)\n",
    "        \n",
    "    summary_op = tf.summary.merge([loss_select_summary,loss_move_summary,loss_summary\n",
    "                                  ,acc_select_summary,acc_move_summary,acc_summary])\n",
    "    \n",
    "    test_select,test_select_summary = get_scatter(\"test_select_loss\")\n",
    "    test_move,test_move_summary = get_scatter(\"test_move_loss\")\n",
    "    test_total,test_total_summary = get_scatter(\"test_total_loss\")\n",
    "    test_selectacc,test_selectacc_summary = get_scatter(\"test_select_acc\")\n",
    "    test_moveacc,test_moveacc_summary = get_scatter(\"test_move_acc\")\n",
    "    test_totalacc,test_totalacc_summary = get_scatter(\"test_total_acc\")\n",
    "    \n",
    "    test_summary_op = tf.summary.merge([test_select_summary,test_move_summary,test_total_summary\n",
    "                                       ,test_selectacc_summary,test_moveacc_summary,test_totalacc_summary])\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "        train_op = optimizer.minimize(loss,global_step=global_step)\n",
    "\n",
    "    train_summary_writer = tf.summary.FileWriter(\"./log/{}_train\".format(model_name), sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.global_step(sess, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??tflearn.layers.residual_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"models/{}\".format(model_name)):\n",
    "    os.mkdir(\"models/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BATCH = 10000\n",
    "N_BATCH_TEST = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 22 STEP 9999 LR 0.001 ACC 75.04 SELACC78.82 MOVACC95.21 LOSS 0.74  100.00 % [==================================================>] 2560000/2560000 \t used:3149s eta:0 sss eta:0 s\n",
      "validating epoch 22 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:54s eta:0 sTEST ACC 0.47125524282455444 SELACC 0.5831900835037231 MOVACC 0.8079296946525574 LOSS 2.4170286655426025\n",
      "\n",
      "EPOCH 23 STEP 9999 LR 0.001 ACC 74.33 SELACC78.43 MOVACC94.77 LOSS 0.76  100.00 % [==================================================>] 2560000/2560000 \t used:5560s eta:0 sss\n",
      "validating epoch 23 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:54s eta:0 sTEST ACC 0.4667249023914337 SELACC 0.5788801908493042 MOVACC 0.8060286641120911 LOSS 2.4456872940063477\n",
      "\n",
      "EPOCH 24 STEP 9999 LR 0.001 ACC 75.16 SELACC79.3 MOVACC94.79 LOSS 0.74  100.00 % [==================================================>] 2560000/2560000 \t used:5559s eta:0 ssss\n",
      "validating epoch 24 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.47514840960502625 SELACC 0.5864583253860474 MOVACC 0.8101171851158142 LOSS 2.481386661529541\n",
      "\n",
      "EPOCH 25 STEP 9999 LR 0.001 ACC 77.37 SELACC80.81 MOVACC95.73 LOSS 0.68  100.00 % [==================================================>] 2560000/2560000 \t used:5562s eta:0 sss\n",
      "validating epoch 25 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.4594808518886566 SELACC 0.574023425579071 MOVACC 0.8003906011581421 LOSS 2.678408622741699\n",
      "\n",
      "EPOCH 26 STEP 9999 LR 0.001 ACC 77.06 SELACC80.59 MOVACC95.62 LOSS 0.69  100.00 % [==================================================>] 2560000/2560000 \t used:5561s eta:0 sss\n",
      "validating epoch 26 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:54s eta:0 sTEST ACC 0.4758685231208801 SELACC 0.5884114503860474 MOVACC 0.80859375 LOSS 2.5563509464263916\n",
      "\n",
      "EPOCH 27 STEP 9999 LR 0.001 ACC 76.82 SELACC80.47 MOVACC95.47 LOSS 0.7  100.00 % [==================================================>] 2560000/2560000 \t used:5556s eta:0 ssss\n",
      "validating epoch 27 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.46077102422714233 SELACC 0.5758333206176758 MOVACC 0.8000390529632568 LOSS 2.7744829654693604\n",
      "\n",
      "EPOCH 28 STEP 9999 LR 0.001 ACC 78.71 SELACC81.58 MOVACC96.48 LOSS 0.63  100.00 % [==================================================>] 2560000/2560000 \t used:5557s eta:0 sss\n",
      "validating epoch 28 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.46656712889671326 SELACC 0.5788021087646484 MOVACC 0.8061718940734863 LOSS 2.853325128555298\n",
      "\n",
      "EPOCH 29 STEP 9999 LR 0.001 ACC 78.16 SELACC81.24 MOVACC96.22 LOSS 0.65  100.00 % [==================================================>] 2560000/2560000 \t used:5561s eta:0 sss\n",
      "validating epoch 29 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.4673483371734619 SELACC 0.5812109112739563 MOVACC 0.8039583563804626 LOSS 2.8683159351348877\n",
      "\n",
      "EPOCH 30 STEP 9999 LR 0.0001 ACC 80.65 SELACC83.27 MOVACC96.85 LOSS 0.58  100.00 % [==================================================>] 2560000/2560000 \t used:5559s eta:0 sss\n",
      "validating epoch 30 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:54s eta:0 sTEST ACC 0.4682302474975586 SELACC 0.5828906297683716 MOVACC 0.8033333420753479 LOSS 2.967599630355835\n",
      "\n",
      "EPOCH 31 STEP 9999 LR 0.0001 ACC 79.84 SELACC82.5 MOVACC96.77 LOSS 0.61  100.00 % [==================================================>] 2560000/2560000 \t used:5554s eta:0 ssss\n",
      "validating epoch 31 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:59s eta:0 sTEST ACC 0.47560566663742065 SELACC 0.5861197710037231 MOVACC 0.8114323019981384 LOSS 2.9342682361602783\n",
      "\n",
      "EPOCH 32 STEP 9999 LR 0.0001 ACC 80.61 SELACC83.16 MOVACC96.93 LOSS 0.59  100.00 % [==================================================>] 2560000/2560000 \t used:5562s eta:0 sss\n",
      "validating epoch 32 batch 9999 100.00 % [==================================================>] 76800/76800 \t used:55s eta:0 sTEST ACC 0.47255992889404297 SELACC 0.5841666460037231 MOVACC 0.8089713454246521 LOSS 3.0034432411193848\n",
      "\n",
      "EPOCH 33 STEP 8654 LR 0.0001 ACC 80.35 SELACC82.84 MOVACC96.99 LOSS 0.59  86.55 % [===========================================>-------] 2215680/2560000 \t used:4814s eta:748 ss"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5f41a00f8fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         _,step_loss,step_acc_move,step_acc_select,step_acc_total,step_value,step_summary = sess.run(\n\u001b[1;32m     49\u001b[0m             [train_op,loss,accuracy_move,accuracy_select,accuracy_total,global_step,summary_op],feed_dict={\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mX1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_lr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             })\n\u001b[1;32m     52\u001b[0m         \u001b[0mtrain_summary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_summary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "restore = True\n",
    "N_EPOCH = 100\n",
    "DECAY_EPOCH = 10\n",
    "\n",
    "class ExpVal:\n",
    "    def __init__(self,exp_a=0.97):\n",
    "        self.val = None\n",
    "        self.exp_a = exp_a\n",
    "    def update(self,newval):\n",
    "        if self.val == None:\n",
    "            self.val = newval\n",
    "        else:\n",
    "            self.val = self.exp_a * self.val + (1 - self.exp_a) * newval\n",
    "    def getval(self):\n",
    "        return round(self.val,2)\n",
    "    \n",
    "expacc = ExpVal()\n",
    "expacc_select = ExpVal()\n",
    "expacc_move = ExpVal()\n",
    "exploss = ExpVal()\n",
    "\n",
    "\n",
    "begining_learning_rate = 1e-1\n",
    "\n",
    "pred_image = None\n",
    "if restore == False:\n",
    "    train_epoch = 1\n",
    "    train_batch = 0\n",
    "for one_epoch in range(train_epoch,N_EPOCH):\n",
    "    train_epoch = one_epoch\n",
    "    pb = ProgressBar(worksum=N_BATCH * BATCH_SIZE,info=\" epoch {} batch {}\".format(train_epoch,train_batch))\n",
    "    pb.startjob()\n",
    "    \n",
    "    for one_batch in range(N_BATCH):\n",
    "        if restore == True and one_batch < train_batch:\n",
    "            pb.auto_display = False\n",
    "            pb.complete(BATCH_SIZE)\n",
    "            pb.auto_display = True\n",
    "            continue\n",
    "        else:\n",
    "            restore = False\n",
    "        train_batch = one_batch\n",
    "        \n",
    "        batch_x1,batch_y1,batch_x2,batch_y2 = trainflow.next()['data']\n",
    "        # learning rate decay strategy\n",
    "        batch_lr = begining_learning_rate * 10 ** -(one_epoch // DECAY_EPOCH)\n",
    "        \n",
    "        _,step_loss,step_acc_move,step_acc_select,step_acc_total,step_value,step_summary = sess.run(\n",
    "            [train_op,loss,accuracy_move,accuracy_select,accuracy_total,global_step,summary_op],feed_dict={\n",
    "                X1:batch_x1,y1:batch_y1,X2:batch_x2,y2:batch_y2,learning_rate:batch_lr,training:True\n",
    "            })\n",
    "        train_summary_writer.add_summary(step_summary,step_value)\n",
    "        step_acc_move *= 100\n",
    "        step_acc_select *= 100\n",
    "        step_acc_total *= 100\n",
    "        expacc.update(step_acc_total)\n",
    "        expacc_select.update(step_acc_select)\n",
    "        expacc_move.update(step_acc_move)\n",
    "        exploss.update(step_loss)\n",
    "\n",
    "       \n",
    "        pb.info = \"EPOCH {} STEP {} LR {} ACC {} SELACC{} MOVACC{} LOSS {} \".format(\n",
    "            one_epoch,one_batch,batch_lr,expacc.getval()\n",
    "            ,expacc_select.getval(),expacc_move.getval(),exploss.getval())\n",
    "        \n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print()\n",
    "    accs = []\n",
    "    accselects = []\n",
    "    accmoves = []\n",
    "    losses = []\n",
    "    lossselects = []\n",
    "    lossmoves = []\n",
    "    pb = ProgressBar(worksum=N_BATCH_TEST * BATCH_SIZE,info=\"validating epoch {} batch {}\".format(train_epoch,train_batch))\n",
    "    pb.startjob()\n",
    "    for one_batch in range(N_BATCH_TEST):\n",
    "        batch_x1,batch_y1,batch_x2,batch_y2 = testflow.next()['data']\n",
    "        step_loss_move,step_loss_select,step_loss,step_accuracy_move,step_accuracy_select,step_accuracy_total = sess.run(\n",
    "            [loss_move,loss_select,loss,accuracy_move,accuracy_select,accuracy_total],feed_dict={\n",
    "                X1:batch_x1,y1:batch_y1,X2:batch_x2,y2:batch_y2,training:False\n",
    "            })\n",
    "        accs.append(step_accuracy_total)\n",
    "        accselects.append(step_accuracy_select)\n",
    "        accmoves.append(step_accuracy_move)\n",
    "        losses.append(step_loss)\n",
    "        lossselects.append(step_loss_select)\n",
    "        lossmoves.append(step_loss_move)\n",
    "        \n",
    "        pb.complete(BATCH_SIZE)\n",
    "    print(\"TEST ACC {} SELACC {} MOVACC {} LOSS {}\".format(np.average(accs),np.average(accselects)\n",
    "                                                           ,np.average(accmoves),np.average(losses)))\n",
    "    #test_select_summary,test_move_summary,test_total_summary\n",
    "    #                                   ,test_selectacc_summary,test_moveacc_summary,test_totalacc_summary\n",
    "    test_to_add_to_log = sess.run(test_summary_op,feed_dict={\n",
    "        test_select:np.average(lossselects),test_move:np.average(lossmoves),test_total:np.average(losses)\n",
    "        ,test_selectacc:np.average(accselects),test_moveacc:np.average(accmoves),test_totalacc:np.average(accs)\n",
    "    })\n",
    "    train_summary_writer.add_summary(test_to_add_to_log,step_value)\n",
    "    print()\n",
    "    saver = tf.train.Saver(var_list=tf.global_variables())\n",
    "    saver.save(sess,\"models/{}/model_{}\".format(model_name,one_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
